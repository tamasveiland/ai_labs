name: Azure AI Evaluations Pipeline

on:
  push:
    branches: [main, develop]
    paths:
      - 'labs/lab03-azure-ai-evaluations/**'
  pull_request:
    branches: [main]
    paths:
      - 'labs/lab03-azure-ai-evaluations/**'
  schedule:
    # Run weekly on Sundays at midnight UTC
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      run_full_evaluation:
        description: 'Run full evaluation suite'
        required: false
        default: 'true'

env:
  PYTHON_VERSION: '3.11'
  WORKING_DIR: 'labs/lab03-azure-ai-evaluations'

jobs:
  evaluate-ai-application:
    name: Evaluate AI Application
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install application dependencies
        working-directory: ${{ env.WORKING_DIR }}/src
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install evaluation dependencies
        working-directory: ${{ env.WORKING_DIR }}/evaluations
        run: |
          pip install -r requirements.txt
      
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      
      - name: Run AI Evaluations
        working-directory: ${{ env.WORKING_DIR }}/evaluations
        env:
          AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
          AZURE_OPENAI_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_DEPLOYMENT_NAME }}
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_SEARCH_ENDPOINT: ${{ secrets.AZURE_SEARCH_ENDPOINT }}
          AZURE_SEARCH_INDEX: documents-index
        run: |
          python evaluate_local.py
      
      - name: Upload evaluation results
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results-${{ github.run_number }}
          path: ${{ env.WORKING_DIR }}/evaluations/evaluation_results/
          retention-days: 30
      
      - name: Analyze results
        working-directory: ${{ env.WORKING_DIR }}/scripts
        run: |
          python analyze_results.py
      
      - name: Upload analysis summary
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-summary-${{ github.run_number }}
          path: ${{ env.WORKING_DIR }}/scripts/evaluation_summary.md
          retention-days: 30
      
      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summaryPath = '${{ env.WORKING_DIR }}/scripts/evaluation_summary.md';
            
            if (fs.existsSync(summaryPath)) {
              const summary = fs.readFileSync(summaryPath, 'utf8');
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ðŸ¤– AI Evaluation Results\n\n${summary}\n\n---\n*Automated evaluation completed in workflow run #${{ github.run_number }}*`
              });
            }
      
      - name: Check quality thresholds
        working-directory: ${{ env.WORKING_DIR }}/scripts
        run: |
          python - <<'EOF'
          import json
          import sys
          from pathlib import Path
          
          # Load results
          results_file = Path("../evaluations/evaluation_results/eval_results.json")
          if not results_file.exists():
              print("âš ï¸  No results file found, skipping threshold check")
              sys.exit(0)
          
          with open(results_file) as f:
              results = json.load(f)
          
          # Define minimum thresholds
          thresholds = {
              "groundedness": 3.0,
              "relevance": 3.0,
              "coherence": 3.5,
              "fluency": 4.0
          }
          
          # Check thresholds
          failed_metrics = []
          if "metrics" in results:
              for metric, threshold in thresholds.items():
                  if metric in results["metrics"]:
                      mean_score = results["metrics"][metric].get("mean", 0)
                      if mean_score < threshold:
                          failed_metrics.append(f"{metric}: {mean_score:.3f} < {threshold}")
          
          # Report results
          if failed_metrics:
              print("âŒ Quality thresholds not met:")
              for failure in failed_metrics:
                  print(f"  - {failure}")
              sys.exit(1)
          else:
              print("âœ… All quality thresholds met!")
              sys.exit(0)
          EOF
      
      - name: Create job summary
        if: always()
        run: |
          echo "## ðŸŽ¯ Evaluation Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run**: #${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "${{ env.WORKING_DIR }}/scripts/evaluation_summary.md" ]; then
            cat "${{ env.WORKING_DIR }}/scripts/evaluation_summary.md" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Evaluation summary not generated" >> $GITHUB_STEP_SUMMARY
          fi

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '${{ env.WORKING_DIR }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
      
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 'trivy-results.sarif'
